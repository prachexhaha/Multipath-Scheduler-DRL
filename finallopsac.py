# -*- coding: utf-8 -*-
"""finalLOPSAC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fFrJikmIkjEINSIPj-jxNaBQfSF21B44
"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import numpy as np
import os
import pandas as pd

n_inputs = 6
n_outputs = 2

class SAC:
def __init__(self):
self.actor = self.build_actor()
self.critic = self.build_critic()
self.target_critic = self.build_critic()

def build_actor(self):
model = tf.keras.Sequential([
Dense(64, activation='relu', input_shape=(n_inputs,)),
Dense(64, activation='relu'),
Dense(n_outputs, activation='softmax')
])
return model

def build_critic(self):
model = tf.keras.Sequential([

36

Dense(64, activation='relu', input_shape=(n_inputs,)),
Dense(64, activation='relu'),
Dense(1)
])
return model

def train(self, X, y, epochs):
for epoch in range(epochs):
with tf.GradientTape() as tape:
actions = self.actor.predict(X)
q_values = self.critic.predict(X)
q_values_target = self.target_critic.predict(X)
q_values_target = q_values_target[:, 0]
q_values_target = tf.stop_gradient(q_values_target)
q_values = q_values[:, 0]
loss = tf.reduce_mean(tf.square(q_values - q_values_target))
gradients = tape.gradient(loss, self.actor.trainable_variables)
self.actor.optimizer.apply_gradients(zip(gradients,

self.actor.trainable_variables))

self.target_critic.set_weights(self.critic.get_weights())

def predict(self, X):
return self.actor.predict(X)

dataset = pd.read_csv('/content/Copy of 1 - FBC.csv')
dataset = dataset.to_numpy()

37

sac = SAC()
sac.train(dataset[:, :6], dataset[:, 6], epochs=100)
predictions = sac.predict(dataset[:, :6])
with open('log.txt', 'w') as f:
for i in range(len(dataset)):
f.write(f'Predicted path: {np.argmax(predictions[i])}, Actual path:

{dataset[i, 6]}\n')
accuracy = np.mean(np.argmax(predictions, axis=1) == dataset[:, 6])
print(f'Accuracy: {accuracy:.2f}')