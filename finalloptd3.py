# -*- coding: utf-8 -*-
"""finalLOPTD3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19yISx_FUHOuxjyOgCcABzir1mM9B371q
"""

import numpy as np
import pandas as pd
from keras.models import Model
from keras.layers import Input, Dense
from keras.optimizers import Adam
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping

# Load the dataset
df = pd.read_csv('/content/Copy of 1 - FBC.csv')

# Define the input and output shapes
input_shape = (8,)
output_shape = (2,)  # 2 possible paths

# Define the actor and critic networks
actor_input = Input(shape=input_shape)
actor_x = Dense(64, activation='relu')(actor_input)
actor_x = Dense(64, activation='relu')(actor_x)
actor_output = Dense(2, activation='softmax')(actor_x)
actor = Model(actor_input, actor_output)

critic_input = Input(shape=input_shape)
critic_x = Dense(64, activation='relu')(critic_input)
critic_x = Dense(64, activation='relu')(critic_x)
critic_output = Dense(1)(critic_x)  # Output a single value
critic = Model(critic_input, critic_output)

# Define the TD3 algorithm
class TD3:
    def __init__(self, actor, critic, gamma=0.99, tau=0.005):
        self.actor = actor
        self.critic = critic
        self.gamma = gamma
        self.tau = tau
        self.actor_optimizer = Adam(lr=0.001)
        self.critic_optimizer = Adam(lr=0.001)

    def train(self, states, actions, rewards, next_states, dones):
        print(rewards.dtype)
        print(dones.dtype)
        #print(type(next_states))

        next_states_array = next_states.select_dtypes(include=[np.number]).values.astype('float32')
        next_states_array = next_states_array.reshape(-1)  # Reshape to 1-dimensional array

        print(next_states_array.dtype)

        rewards = rewards.astype('float32')
        dones = dones.astype('float32')
        next_states_array=next_states_array.astype('float32')
        # Compute the target values
        target_values = rewards + self.gamma * (1 - np.array(dones)) * self.critic.predict(next_states_array)

        # Update the critic
        self.critic_optimizer.zero_grad()
        critic_loss = self.critic.compile(loss='mse', optimizer=self.critic_optimizer)
        critic_loss.fit(states, target_values, epochs=1, verbose=0)

        # Update the actor
        self.actor_optimizer.zero_grad()
        actor_loss = -self.critic.predict(states)
        actor_loss = actor_loss.mean()
        actor_loss.backward()
        self.actor_optimizer.step()

        # Update the target networks
        self.update_target_networks()

    def update_target_networks(self):
        actor_weights = self.actor.get_weights()
        critic_weights = self.critic.get_weights()
        for i in range(len(actor_weights)):
            actor_weights[i] = self.tau * actor_weights[i] + (1 - self.tau) * actor_weights[i]
            critic_weights[i] = self.tau * critic_weights[i] + (1 - self.tau) * critic_weights[i]
        self.actor.set_weights(actor_weights)
        self.critic.set_weights(critic_weights)

    def predict(self, states):
        return self.actor.predict(states)

# Create the TD3 algorithm
td3 = TD3(actor, critic)

# Train the model
batch_size = 32
for episode in range(1000):
    batch_states = df.sample(batch_size)
    batch_actions = batch_states['selected path']
    batch_rewards = batch_states['reward']
    batch_next_states = batch_states
    batch_dones = np.zeros((batch_size,))
    td3.train(batch_states.drop('selected path', axis=1), batch_actions, batch_rewards, batch_next_states.drop('selected path', axis=1), batch_dones)

    # Log the predicted selected path, actual selected path, and accuracy
    predicted_paths = td3.predict(batch_states.drop('selected path', axis=1))
    predicted_paths = np.argmax(predicted_paths, axis=1)
    actual_paths = batch_actions
    accuracy = np.mean(predicted_paths == actual_paths)
    print(f'Episode {episode}, Accuracy: {accuracy:.2f}')

    # Evaluate the model using a holdout set
    if episode % 100 == 0:
        holdout_states = df.sample(100)
        holdout_actions = holdout_states['selected path']
        holdout_rewards = holdout_states['reward']
        holdout_next_states = holdout_states
        holdout_dones = np.zeros((100,))
        holdout_predicted_paths = td3.predict(holdout_states.drop('selected path', axis=1))
        holdout_predicted_paths = np.argmax(holdout_predicted_paths, axis=1)
        holdout_accuracy = np.mean(holdout_predicted_paths == holdout_actions)
        print(f'Holdout Accuracy: {holdout_accuracy:.2f}')

    # Stop training if the desired accuracy is reached
    if accuracy > 0.95:
        break