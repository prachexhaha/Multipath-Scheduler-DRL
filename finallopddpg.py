# -*- coding: utf-8 -*-
"""FinalLOPDDPG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1427Mb4fehHeLys_w3sOw2dLP5dFjopdS
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models

# Define the Actor network
def build_actor_network(input_shape, num_actions):
    model = models.Sequential([
        layers.Dense(64, activation='relu', input_shape=input_shape),
        layers.Dense(64, activation='relu'),
        layers.Dense(num_actions, activation='softmax')
    ])
    return model

# Define the Critic network
def build_critic_network(input_shape, num_actions):
    state_input = layers.Input(shape=input_shape)
    action_input = layers.Input(shape=(num_actions,))

    # Define state pathway
    state_pathway = layers.Dense(64, activation='relu')(state_input)

    # Define action pathway
    action_pathway = layers.Dense(64, activation='relu')(action_input)

    # Combine state and action pathways
    combined = layers.Concatenate()([state_pathway, action_pathway])
    combined = layers.Dense(64, activation='relu')(combined)

    # Output Q-value
    output = layers.Dense(1)(combined)

    model = models.Model(inputs=[state_input, action_input], outputs=output)
    return model


# Define the Replay Buffer
class ReplayBuffer:
    def __init__(self, buffer_size):
        self.buffer_size = buffer_size
        self.buffer = []

    def add(self, experience):
        self.buffer.append(experience)
        if len(self.buffer) > self.buffer_size:
            self.buffer.pop(0)

    def sample(self, batch_size):
        batch = np.random.choice(len(self.buffer), batch_size, replace=False)
        states, actions, rewards, next_states, done_flags = zip(*[self.buffer[idx] for idx in batch])
        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(done_flags)

# Define the DDPG Agent
class DDPGAgent:
    def __init__(self, input_shape, num_actions, lr_actor=0.001, lr_critic=0.001, batch_size=32):
        self.input_shape = input_shape
        self.num_actions = num_actions
        self.lr_actor = lr_actor
        self.lr_critic = lr_critic
        self.batch_size = batch_size

        self.actor_network = build_actor_network(input_shape, num_actions)
        self.critic_network = build_critic_network(input_shape, num_actions)
        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr_actor)
        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr_critic)
        self.replay_buffer = ReplayBuffer(buffer_size=10000)

    def choose_action(self, state):
      state = np.array(state)  # Convert state to NumPy array
      print("State dtype:", state.dtype)  # Check data type of state
      print("State values:", state)  # Check values in state array
      state = state.reshape(1, -1)  # Reshape state for input to the network
      print("State shape:", state.shape)  # Check shape of state
      print("Actor network weights:", self.actor_network.get_weights())  # Check weights in actor network
      action_probs = self.actor_network.predict(state)[0]
      print("Action Probabilities:", action_probs)  # Print action probabilities
      return np.random.choice(self.num_actions, p=action_probs)



    def remember(self, state, action, reward, next_state, done):
        self.replay_buffer.add((state, action, reward, next_state, done))

    def train(self):
        states, actions, rewards, next_states, done_flags = self.replay_buffer.sample(self.batch_size)

        with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:
            # Compute actor loss
            actions_pred = self.actor_network(states)
            actor_loss = -tf.math.log(tf.reduce_sum(actions_pred * tf.one_hot(actions, self.num_actions), axis=1))

            # Compute critic loss
            target_actions = self.actor_network(next_states)
            q_values = self.critic_network([next_states, target_actions])
            target_q_values = rewards + (1 - done_flags) * 0.99 * q_values
            critic_loss = tf.reduce_mean(tf.square(target_q_values - self.critic_network([states, actions])))

        # Update actor network
        actor_gradients = actor_tape.gradient(actor_loss, self.actor_network.trainable_variables)
        self.actor_optimizer.apply_gradients(zip(actor_gradients, self.actor_network.trainable_variables))

        # Update critic network
        critic_gradients = critic_tape.gradient(critic_loss, self.critic_network.trainable_variables)
        self.critic_optimizer.apply_gradients(zip(critic_gradients, self.critic_network.trainable_variables))

# Load data from CSV
data = pd.read_csv("/content/Copy of 1 - FBC.csv")

# Assuming the last column is the target label
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

# Instantiate the DDPG Agent
input_shape = X.shape[1:]
num_actions = len(np.unique(y))
agent = DDPGAgent(input_shape, num_actions)

# Training Loop
num_epochs = 100  # You can adjust this
for epoch in range(num_epochs):
    for i in range(len(X)):
        state = X[i]
        action = agent.choose_action(state)
        next_state = X[i+1] if i+1 < len(X) else state
        reward = 1 if action == y[i] else -1
        done = False if i+1 < len(X) else True
        agent.remember(state, action, reward, next_state, done)
        agent.train()

# Evaluation on holdout set
# Assuming you have a holdout set in X_holdout and y_holdout
correct_predictions = 0
for i in range(len(X_holdout)):
    state = X_holdout[i]
    action = agent.choose_action(state)
    if action == y_holdout[i]:
        correct_predictions += 1
accuracy = correct_predictions / len(X_holdout)
print("Accuracy on holdout set:", accuracy)